{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:10:37.479335Z",
     "start_time": "2025-03-11T21:10:37.476047Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T21:10:52.634892Z",
     "start_time": "2025-03-11T21:10:45.856473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06b417549664081b1a2d3b943bd8bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce714a330af4ece9f26942ec92f9f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List1: 'The quick brown fox jumps over the lazy dog.'\n",
      "Best match in List2: 'A fast, dark-colored fox leaps above a sleeping canine.'\n",
      "Cosine similarity: 0.7404\n",
      "--------------------------------------------------------------------------------\n",
      "List1: 'Machine learning is fascinating.'\n",
      "Best match in List2: 'I find AI to be very interesting.'\n",
      "Cosine similarity: 0.5942\n",
      "--------------------------------------------------------------------------------\n",
      "List1: 'Artificial intelligence is transforming the world.'\n",
      "Best match in List2: 'The study of machine intelligence is evolving rapidly.'\n",
      "Cosine similarity: 0.6928\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example lists of strings (replace these with your actual data)\n",
    "list1 = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"Artificial intelligence is transforming the world.\"\n",
    "]\n",
    "list2 = [\n",
    "    \"A fast, dark-colored fox leaps above a sleeping canine.\",\n",
    "    \"I find AI to be very interesting.\",\n",
    "    \"The study of machine intelligence is evolving rapidly.\"\n",
    "]\n",
    "\n",
    "# Load the pre-trained SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode both lists using batch processing for efficiency\n",
    "embeddings1 = model.encode(list1, batch_size=32, show_progress_bar=True)\n",
    "embeddings2 = model.encode(list2, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarity matrix between embeddings1 and embeddings2\n",
    "# Each entry [i, j] corresponds to the cosine similarity between list1[i] and list2[j]\n",
    "cos_sim_matrix = cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "# For each string in list1, identify the best matching string in list2\n",
    "best_match_indices = np.argmax(cos_sim_matrix, axis=1)\n",
    "best_match_scores = np.max(cos_sim_matrix, axis=1)\n",
    "\n",
    "# Display the best matches and their cosine similarity scores\n",
    "for idx, match_idx in enumerate(best_match_indices):\n",
    "    print(f\"List1: '{list1[idx]}'\")\n",
    "    print(f\"Best match in List2: '{list2[match_idx]}'\")\n",
    "    print(f\"Cosine similarity: {best_match_scores[idx]:.4f}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:20:26.358266Z",
     "start_time": "2025-03-12T17:20:26.309641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Matches (Function 1 output):\n",
      "  Series1_Index                                       Series1_Text  \\\n",
      "0             a       The quick brown fox jumps over the lazy dog.   \n",
      "4             c  Artificial intelligence is transforming the wo...   \n",
      "1             b                   Machine learning is fascinating.   \n",
      "2             b                   Machine learning is fascinating.   \n",
      "3             c  Artificial intelligence is transforming the wo...   \n",
      "\n",
      "  Series2_Index                                       Series2_Text  \\\n",
      "0             x  A fast, dark-colored fox leaps above a sleepin...   \n",
      "4             z  The study of machine intelligence is evolving ...   \n",
      "1             y                  I find AI to be very interesting.   \n",
      "2             z  The study of machine intelligence is evolving ...   \n",
      "3             y                  I find AI to be very interesting.   \n",
      "\n",
      "   Cosine_Similarity  \n",
      "0           0.740368  \n",
      "4           0.692750  \n",
      "1           0.594170  \n",
      "2           0.569290  \n",
      "3           0.566550  \n",
      "\n",
      "Groups (Function 2 output):\n",
      "Group 1:\n",
      "  a: The quick brown fox jumps over the lazy dog.\n",
      "  x: A fast, dark-colored fox leaps above a sleeping canine.\n",
      "Group 2:\n",
      "  b: Machine learning is fascinating.\n",
      "  c: Artificial intelligence is transforming the world.\n",
      "  y: I find AI to be very interesting.\n",
      "  z: The study of machine intelligence is evolving rapidly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def get_all_matches(series1, series2, emb1, emb2, th=0.8):\n",
    "    \"\"\"\n",
    "    Given two pandas Series (series1 and series2) along with their precomputed embeddings (emb1 and emb2),\n",
    "    compute the cosine similarity between every string from series1 and every string from series2.\n",
    "    \n",
    "    Returns a DataFrame with all pairs (one string from each Series) having a cosine similarity >= th.\n",
    "    Each row includes:\n",
    "      - Series1_Index, Series1_Text,\n",
    "      - Series2_Index, Series2_Text,\n",
    "      - Cosine_Similarity.\n",
    "    \n",
    "    Parameters:\n",
    "      series1 (pd.Series): Series of strings (with index) for list1.\n",
    "      series2 (pd.Series): Series of strings (with index) for list2.\n",
    "      emb1 (np.ndarray): Embeddings for the strings in series1 (assumed to correspond in order to series1.index).\n",
    "      emb2 (np.ndarray): Embeddings for the strings in series2.\n",
    "      th (float): Cosine similarity threshold (default=0.8).\n",
    "    \n",
    "    Returns:\n",
    "      pd.DataFrame: DataFrame containing all unique pairs with similarity >= th.\n",
    "    \"\"\"\n",
    "    # Compute the cosine similarity matrix between emb1 and emb2.\n",
    "    sim_matrix = cosine_similarity(emb1, emb2)\n",
    "    \n",
    "    pairs = []\n",
    "    # Loop over rows (series1) and columns (series2) to collect pairs meeting threshold.\n",
    "    n1, n2 = sim_matrix.shape\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            sim = sim_matrix[i, j]\n",
    "            if sim >= th:\n",
    "                pairs.append({\n",
    "                    \"Series1_Index\": series1.index[i],\n",
    "                    \"Series1_Text\": series1.iloc[i],\n",
    "                    \"Series2_Index\": series2.index[j],\n",
    "                    \"Series2_Text\": series2.iloc[j],\n",
    "                    \"Cosine_Similarity\": sim\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame of matches.\n",
    "    matches_df = pd.DataFrame(pairs)\n",
    "    # (If the same pair could be found twice by any chance, drop duplicates.)\n",
    "    matches_df.drop_duplicates(inplace=True)\n",
    "    # Optionally, sort by descending similarity.\n",
    "    matches_df.sort_values(by=\"Cosine_Similarity\", ascending=False, inplace=True)\n",
    "    return matches_df\n",
    "\n",
    "\n",
    "def group_matches(match_df, series1, series2, emb1, emb2, th=0.8):\n",
    "    \"\"\"\n",
    "    From the DataFrame of all matches (obtained via get_all_matches), create groups among the involved strings \n",
    "    (from both series). Each group will contain strings (from series1 and series2) such that every pair of strings in \n",
    "    the group has a cosine similarity >= th. Groups are disjoint (i.e. a string appears in only one group).\n",
    "    \n",
    "    Each member in the output now also includes a \"Source\" key indicating which series the string came from.\n",
    "    \n",
    "    Returns:\n",
    "      dict: A dictionary where each key is a group label and the corresponding value is a list of dicts,\n",
    "            each with keys \"Index\", \"Text\", and \"Source\".\n",
    "    \n",
    "    Parameters:\n",
    "      match_df (pd.DataFrame): DataFrame output from get_all_matches.\n",
    "      series1 (pd.Series): Original Series for list1.\n",
    "      series2 (pd.Series): Original Series for list2.\n",
    "      emb1 (np.ndarray): Embeddings for series1.\n",
    "      emb2 (np.ndarray): Embeddings for series2.\n",
    "      th (float): Cosine similarity threshold (default=0.8).\n",
    "    \"\"\"\n",
    "    # Create a lookup dictionary mapping index -> (text, embedding, source)\n",
    "    emb_dict = {}\n",
    "    for i, idx in enumerate(series1.index):\n",
    "        emb_dict[idx] = {\"Text\": series1.iloc[i], \"Embedding\": emb1[i], \"Source\": \"series1\"}\n",
    "    for i, idx in enumerate(series2.index):\n",
    "        emb_dict[idx] = {\"Text\": series2.iloc[i], \"Embedding\": emb2[i], \"Source\": \"series2\"}\n",
    "    \n",
    "    # Get the union of indices that appear in match_df.\n",
    "    indices1 = match_df[\"Series1_Index\"].unique()\n",
    "    indices2 = match_df[\"Series2_Index\"].unique()\n",
    "    union_indices = set(indices1).union(set(indices2))\n",
    "    \n",
    "    # Build a list of nodes (with index, text, embedding, and source) in a consistent order.\n",
    "    nodes = []\n",
    "    for idx in union_indices:\n",
    "        if idx in emb_dict:\n",
    "            nodes.append({\n",
    "                \"Index\": idx,\n",
    "                \"Text\": emb_dict[idx][\"Text\"],\n",
    "                \"Embedding\": emb_dict[idx][\"Embedding\"],\n",
    "                \"Source\": emb_dict[idx][\"Source\"]\n",
    "            })\n",
    "    # Ensure a consistent order (for example, alphabetical by index)\n",
    "    nodes = sorted(nodes, key=lambda x: x[\"Index\"])\n",
    "    \n",
    "    if not nodes:\n",
    "        return {}\n",
    "    \n",
    "    # Create an array of embeddings for these nodes.\n",
    "    X = np.array([node[\"Embedding\"] for node in nodes])\n",
    "    \n",
    "    # Compute the condensed pairwise distance matrix using cosine distance (which is 1 - cosine similarity).\n",
    "    dist_condensed = pdist(X, metric='cosine')\n",
    "    \n",
    "    # Perform complete linkage clustering.\n",
    "    # With threshold th on cosine similarity, the maximum allowable distance is 1 - th.\n",
    "    Z = linkage(dist_condensed, method='complete')\n",
    "    \n",
    "    # Form flat clusters: any two nodes in a cluster will have complete-linkage distance <= (1 - th)\n",
    "    cluster_labels = fcluster(Z, t=1 - th, criterion='distance')\n",
    "    \n",
    "    # Group the nodes by their cluster labels.\n",
    "    groups = {}\n",
    "    for label, node in zip(cluster_labels, nodes):\n",
    "        groups.setdefault(label, []).append({\n",
    "            \"Index\": node[\"Index\"],\n",
    "            \"Text\": node[\"Text\"],\n",
    "            \"Source\": node[\"Source\"]\n",
    "        })\n",
    "    \n",
    "    return groups\n",
    "\n",
    "# === Example Usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Example Series data (with indices)\n",
    "    series1 = pd.Series({\n",
    "        'a': \"The quick brown fox jumps over the lazy dog.\",\n",
    "        'b': \"Machine learning is fascinating.\",\n",
    "        'c': \"Artificial intelligence is transforming the world.\"\n",
    "    })\n",
    "    series2 = pd.Series({\n",
    "        'x': \"A fast, dark-colored fox leaps above a sleeping canine.\",\n",
    "        'y': \"I find AI to be very interesting.\",\n",
    "        'z': \"The study of machine intelligence is evolving rapidly.\"\n",
    "    })\n",
    "    \n",
    "    # For demonstration, let's assume we have precomputed embeddings.\n",
    "    # In practice, these would be computed via an SBERT model.\n",
    "    # Here we simulate embeddings with random vectors (for reproducibility, set a seed).\n",
    "    np.random.seed(42)\n",
    "    emb1 = model.encode(series1.values)  # e.g. 768-dimensional embeddings\n",
    "    emb2 = model.encode(series2.values)\n",
    "    \n",
    "    # For a realistic test, you would use a SentenceTransformer to compute emb1 and emb2.\n",
    "    \n",
    "    # Function 1: Get all matches with cosine similarity >= 0.8.\n",
    "    # (With random embeddings, you likely wonâ€™t get any matches above 0.8.\n",
    "    # For demonstration, you might adjust the threshold or use real embeddings.)\n",
    "    matches_df = get_all_matches(series1, series2, emb1, emb2, th=0.5)\n",
    "    print(\"All Matches (Function 1 output):\")\n",
    "    print(matches_df)\n",
    "    \n",
    "    # Function 2: Create groups from the matches.\n",
    "    groups = group_matches(matches_df, series1, series2, emb1, emb2, th=0.5)\n",
    "    print(\"\\nGroups (Function 2 output):\")\n",
    "    for group_label, members in groups.items():\n",
    "        print(f\"Group {group_label}:\")\n",
    "        for member in members:\n",
    "            print(f\"  {member['Index']}: {member['Text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:20:32.495716Z",
     "start_time": "2025-03-12T17:20:32.490968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [{'Index': 'a',\n",
       "   'Text': 'The quick brown fox jumps over the lazy dog.',\n",
       "   'Source': 'series1'},\n",
       "  {'Index': 'x',\n",
       "   'Text': 'A fast, dark-colored fox leaps above a sleeping canine.',\n",
       "   'Source': 'series2'}],\n",
       " 2: [{'Index': 'b',\n",
       "   'Text': 'Machine learning is fascinating.',\n",
       "   'Source': 'series1'},\n",
       "  {'Index': 'c',\n",
       "   'Text': 'Artificial intelligence is transforming the world.',\n",
       "   'Source': 'series1'},\n",
       "  {'Index': 'y',\n",
       "   'Text': 'I find AI to be very interesting.',\n",
       "   'Source': 'series2'},\n",
       "  {'Index': 'z',\n",
       "   'Text': 'The study of machine intelligence is evolving rapidly.',\n",
       "   'Source': 'series2'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CARVA)",
   "language": "python",
   "name": "carva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
