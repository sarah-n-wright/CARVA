{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Curation Notebook A - Processing of GWAS and RV Data\n",
    "\n",
    "- Filter CV and RV studies\n",
    "- Filter gene associations\n",
    "- Convert to NCBI Gene IDs\n",
    "- Clean the trait EFO codes\n",
    "\n",
    "Figures generated in this notebook:\n",
    "- None\n",
    "\n",
    "Datasets generated in this notebook:\n",
    "- Supplemental Table 1 - Common and Rare Variant study information and associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:15:17.517745Z",
     "start_time": "2025-07-03T17:15:17.514985Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from matplotlib_venn import venn2, venn3\n",
    "from neteval import gene_mapper as gm\n",
    "from neteval import query_ensembl as qe\n",
    "from neteval import query_hgnc as qh\n",
    "import obonet as obo\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:15:17.477192Z",
     "start_time": "2025-07-03T17:14:57.283908Z"
    }
   },
   "outputs": [],
   "source": [
    "import sentence_transformers\n",
    "model = sentence_transformers.SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T16:46:56.345669Z",
     "start_time": "2025-07-03T16:46:56.344005Z"
    }
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "datadir = os.path.join(cwd, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T16:46:57.843951Z",
     "start_time": "2025-07-03T16:46:57.812296Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "arial_font_path = os.path.join(datadir, 'Reference_Data', 'Arial.TTF')\n",
    "fm.fontManager.addfont(arial_font_path)\n",
    "matplotlib.rcParams['font.family'] = 'Arial'\n",
    "matplotlib.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "plt.rcParams['axes.linewidth'] = 0.5\n",
    "plt.rcParams['hatch.linewidth'] = 0.5\n",
    "plt.rcParams['xtick.major.width'] = 0.4\n",
    "plt.rcParams['ytick.major.width'] = 0.4\n",
    "plt.rcParams['xtick.minor.width'] = 0.3\n",
    "plt.rcParams['ytick.minor.width'] = 0.3\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "plt.rcParams['xtick.major.size'] = 3\n",
    "plt.rcParams['ytick.major.size'] = 3\n",
    "plt.rcParams['xtick.minor.size'] = 2\n",
    "plt.rcParams['ytick.minor.size'] = 2\n",
    "plt.rcParams['xtick.major.pad'] = 1\n",
    "plt.rcParams['ytick.major.pad'] = 1\n",
    "plt.rcParams['axes.labelpad'] = 1\n",
    "plt.rcParams['patch.linewidth'] = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GWAS Catalog - Common Variant Associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:01:27.968965Z",
     "start_time": "2025-07-03T17:01:26.835293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Studies: 138810\n",
      "- Studies with background trait: 124463\n",
      "- Not genome-wide studies: 101460\n",
      "- Studies with missing trait information: 101402\n",
      "- Studies with multiple traits: 95859\n",
      "- Studies with at < 3 associations: 19452\n",
      "Total filtered studies 19452\n"
     ]
    }
   ],
   "source": [
    "# Get study information\n",
    "study_info = pd.read_csv(os.path.join(datadir,'Reference_Data', 'gwas-catalog-v1.0.3.1-studies-r2025-03-26.tsv.gz'), \n",
    "                         sep='\\t', low_memory=False)\n",
    "print(f'Initial Studies:', len(study_info))\n",
    "\n",
    "# Remove studies with background traits\n",
    "study_info = study_info[study_info['MAPPED BACKGROUND TRAIT'].isna()]\n",
    "print(f'- Studies with background trait:', len(study_info))\n",
    "\n",
    "# Use only genome-wide studies\n",
    "study_info['GENOTYPING TECHNOLOGY'] = study_info['GENOTYPING TECHNOLOGY'].apply(lambda x: x.split('[')[0].strip())\n",
    "study_info = study_info[study_info['GENOTYPING TECHNOLOGY'].isin(['Genome-wide genotyping array', \n",
    "       'Genome-wide genotyping array, Genome-wide sequencing',])]\n",
    "print(f'- Not genome-wide studies:', len(study_info))\n",
    "\n",
    "# Remove studies with missing trait information\n",
    "study_info = study_info.dropna(subset=['DISEASE/TRAIT', 'MAPPED_TRAIT', 'MAPPED_TRAIT_URI'])\n",
    "print(f'- Studies with missing trait information:', len(study_info))\n",
    "\n",
    "# Remove studies mapped to multiple traits\n",
    "study_info = study_info[~study_info['MAPPED_TRAIT_URI'].str.contains(', ')]\n",
    "print(f'- Studies with multiple traits:', len(study_info))\n",
    "\n",
    "# Keep studies with at least 3 associations (will be filtered based on genes later)\n",
    "study_info = study_info[study_info['ASSOCIATION COUNT'] >= 3]\n",
    "print(f'- Studies with at < 3 associations:', len(study_info))\n",
    "print(f'Total filtered studies:', len(study_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean trait information\n",
    "\n",
    "Mappings between trait descriptions and EFO terms are in the GWAS Catalog are not one-to-one. Therefore, we will take the best matched EFO term for each unique trait description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:04:47.596658Z",
     "start_time": "2025-07-03T17:04:47.389478Z"
    }
   },
   "outputs": [],
   "source": [
    "trait_info = pd.read_csv(os.path.join(datadir,'Reference_Data',  'gwas_catalog_trait-mappings_r2025-03-26.tsv.gz'), sep='\\t')\n",
    "\n",
    "# Identify traits present in filtered studies, and the number of occurences of each trait\n",
    "trait_info = trait_info[trait_info['Disease trait'].isin(study_info['DISEASE/TRAIT'])]\n",
    "trait_disease_counts = trait_info['Disease trait'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:04:48.095907Z",
     "start_time": "2025-07-03T17:04:48.090529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Identify one-to-one mappings\n",
    "one_match = trait_disease_counts[trait_disease_counts == 1].index\n",
    "one_match = trait_info[trait_info['Disease trait'].isin(one_match)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:06:04.618171Z",
     "start_time": "2025-07-03T17:06:04.127017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate dictionary of trait mappings\n",
    "match_dict = {}\n",
    "for i, row in one_match.iterrows():\n",
    "    match_dict[row['Disease trait']] = (row['EFO term'], row['EFO URI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best matches for traits with multiple mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:16:10.262002Z",
     "start_time": "2025-07-03T17:16:10.257772Z"
    }
   },
   "outputs": [],
   "source": [
    "multi_match = trait_disease_counts[trait_disease_counts > 2].index\n",
    "multi_match = trait_info[trait_info['Disease trait'].isin(multi_match)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:03:01.534661Z",
     "start_time": "2025-07-03T18:03:01.529523Z"
    },
    "code_folding": [
     1,
     26
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Sentence embedding cannot always capture acronyms, so need to expand the most common ones\n",
    "def replace_acronyms(text, acronym_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    Replace acronyms in a string with their expansions followed by the original acronym in parentheses.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input string containing acronyms.\n",
    "        acronym_dict (dict): A dictionary where keys are acronyms (str) and values are the expansions (str).\n",
    "    \n",
    "    Returns:\n",
    "        str: The text with acronyms replaced by their expansion and the original acronym in parentheses.\n",
    "    \"\"\"\n",
    "    # This regex matches words consisting of at least two uppercase letters.\n",
    "    acronym_pattern = re.compile(r'[A-Z]{2,}')\n",
    "    \n",
    "    def replacer(match):\n",
    "        word = match.group(0)\n",
    "        if word in acronym_dict:\n",
    "            # Return the expansion with the original acronym in parentheses.\n",
    "            return f\"{acronym_dict[word]} ({word})\"\n",
    "        else:\n",
    "            return word\n",
    "    \n",
    "    return acronym_pattern.sub(replacer, text)\n",
    "\n",
    "def extract_acronyms(strings):\n",
    "    \"\"\"\n",
    "    Extract acronyms from a list of strings and count their occurrences.\n",
    "    Acronyms are identified as words that consist entirely of uppercase letters.\n",
    "    \n",
    "    Args:\n",
    "        strings (list of str): List of strings to search for acronyms.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with acronyms as keys and their counts as values.\n",
    "    \"\"\"\n",
    "    acronym_pattern = re.compile(r'\\b[A-Z]{2,}\\b')\n",
    "    acronym_counts = {}\n",
    "    for text in strings:\n",
    "        found = acronym_pattern.findall(text)\n",
    "        for acronym in found:\n",
    "            acronym_counts[acronym] = acronym_counts.get(acronym, 0) + 1\n",
    "    return acronym_counts\n",
    "\n",
    "\n",
    "def map_trait(trait_str, trait_info, usecol='Disease trait'):\n",
    "    options = trait_info[trait_info[usecol]==trait_str]\n",
    "    # embeddings\n",
    "    trait_embedding = model.encode([trait_str.lower().replace('levels', 'measurement')])\n",
    "    option_embeddings = model.encode([x.lower() for x in options['EFO term'].values])\n",
    "    # cosine similarity\n",
    "    similarities = cosine_similarity(trait_embedding, option_embeddings)\n",
    "    best_option = options.iloc[np.argmax(similarities)]\n",
    "    return (best_option['EFO term'], best_option['EFO URI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:14:23.936979Z",
     "start_time": "2025-07-03T17:14:23.931140Z"
    },
    "code_folding": [
     0,
     17
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "common_acronyms = {\n",
    "    'LDL': 'low density lipoprotein',\n",
    "    'HDL': 'high density lipoprotein',\n",
    "    'BMI': 'body mass index',\n",
    "    'VLDL': 'very low density lipoprotein',\n",
    "    'IL': 'interleukin',\n",
    "    'IDL': 'intermediate density lipoprotein',\n",
    "    'FVC': 'forced vital capacity',\n",
    "    'FEV': 'forced expiratory volume',\n",
    "    'COVID': 'coronavirus',\n",
    "    'HIV': 'human immunodeficiency virus',\n",
    "    'SARS': 'severe acute respiratory syndrome',\n",
    "    'APOE': 'apolipoprotein E',\n",
    "    'APOB': 'apolipoprotein B',\n",
    "    'APOA': 'apolipoprotein A',\n",
    "}\n",
    "\n",
    "acronym_expansions = {\n",
    "    'ldl': 'low density lipoprotein',\n",
    "    'hdl': 'high density lipoprotein',\n",
    "    \"FAW3\": 'omega-3 polyunsaturated fatty acid',\n",
    "    \"FAW6\": 'omega-6 polyunsaturated fatty acid',\n",
    "    \"FEC\": \"Forced Expiratory volume\",\n",
    "    \"FEV\": \"Forced Expiratory volume\",\n",
    "    \"HIV\": \"Human Immunodeficiency Virus\",\n",
    "    \"IV\": \"4\",\n",
    "    \"LDH\": \"lumbar disc herniation\",\n",
    "    \"NHDL\": 'non High density ipoprotein',\n",
    "    \"NSAID\": 'nonsteroidal anti inflammatory drug',\n",
    "    \"ACACE\": \"Acetoacetate\",                                \n",
    "    \"ACE\": \"Acetate\",             \n",
    "    \"ALA\": \"Alanine\",                       \n",
    "    \"ALB\": \"Albumin\",  \n",
    "    \"APOB\": \"Apolipoprotein B\",  \n",
    "    \"APOC\": \"Apolipoprotein C\",  \n",
    "    \"BMI\": \"Body Mass Index\",   \n",
    "    \"BP\": \"Blood Pressure\",   \n",
    "    \"CHOLA\": \"Cholesterol\",    \n",
    "    \"CIT\": \"Citrate\",      \n",
    "    \"CRP\": \"C-Reactive Protein\",   \n",
    "    \"DHA\": \"Docosahexaenoic Acid\",  \n",
    "    \"DISTRIB\": \"Distribution\",     \n",
    "    \"EGFR\": \"Estimated Glomerular Filtration Rate\",   \n",
    "    \"FRAC\": \"Fraction\",                              \n",
    "    \"FVC\": \"Forced Vital Capacity\",               \n",
    "    \"GLN\": \"Glutamine\",                            \n",
    "    \"GLOL\": \"Glycerol\",                          \n",
    "    \"GLY\": \"Glycine\",                             \n",
    "    \"GP\": \"Glycoproteins\",                        \n",
    "    \"HDL\": \"High Density Lipoprotein\",                  \n",
    "    \"HDLC\": \"High Density Lipoprotein Cholesterol\",     \n",
    "    \"HEEL\": \"Heel Bone Mineral Density\",  \n",
    "    \"HEIGHT\": \"Height\",                         \n",
    "    \"HIS\": \"Histidine\",     \n",
    "    \"IDL\": \"Intermediate Density Lipoprotein\",  \n",
    "    \"IGF\": \"Insulin like Growth Factor\",  \n",
    "    \"III\": \"Type III\",     \n",
    "    \"INS\": \"Insulin\",  \n",
    "    \"LA\": \"Linoleic Acid\",   \n",
    "    \"LDL\": \"Low Density Lipoprotein\",\n",
    "    \"LDLC\": \"Low Density Lipoprotein Cholesterol\",\n",
    "    \"LIGHT\": \"Light Scatter\",\n",
    "    \"MUFA\": \"Monounsaturated Fatty Acids\",\n",
    "    \"PC\": \"Phosphatidylcholine\",          \n",
    "    \"PHE\": \"Phenylalanine\",               \n",
    "    \"PUFA\": \"Polyunsaturated Fatty Acids\",\n",
    "    \"PYR\": \"Pyruvate\",                    \n",
    "    \"RBC\": \"Red Blood Cell Count\",        \n",
    "    \"SCZD\": \"Schizophrenia\",              \n",
    "    \"SHBG\": \"Sex Hormone-Binding Globulin\",\n",
    "    \"SPHERED\": \"Spherical Diameter\",       \n",
    "    \"SYST\": \"Systolic Blood Pressure\",     \n",
    "    \"TOTCHO\": \"Total Cholines\",            \n",
    "    \"TOTCHOL\": \"Total Cholesterol\",        \n",
    "    \"TOTPG\": \"Total Phosphoglycerides\",    \n",
    "    \"TSCORE\": \"T-Score\",                   \n",
    "    \"TYR\": \"Tyrosine\",                     \n",
    "    \"VAL\": \"Valine\",                       \n",
    "    \"VIT\": \"Vitamin (unspecified)\",      \n",
    "    \"VLDL\": \"Very Low Density Lipoprotein\", \n",
    "    \"VLDLPL\": \"Very low density lipoprotein Phospholipids\",  \n",
    "    \"VLDLTG\": \"Very low density lipoprotein Triglycerides\",  \n",
    "    \"VOL\": \"Volume\",            \n",
    "    \"Whr\": \"Waist Hip Ratio\", \n",
    "    \"whr\": \"Waist Hip Ratio\",\n",
    "    \"WHR\": \"Waist Hip Ratio\",            \n",
    "    \"XL\": \"Extra Large\",                 \n",
    "    \"XS\": \"Extra Small\"                  \n",
    "}\n",
    "\n",
    "all_acronyms = {**common_acronyms, **acronym_expansions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:14:46.330336Z",
     "start_time": "2025-07-03T17:14:46.210921Z"
    }
   },
   "outputs": [],
   "source": [
    "# expand the acronymns in both data sets\n",
    "study_info['TraitExp'] =study_info['DISEASE/TRAIT'].apply(lambda x: replace_acronyms(x, all_acronyms))\n",
    "study_info['MappedExp'] = study_info['MAPPED_TRAIT'].apply(lambda x: replace_acronyms(x, all_acronyms))\n",
    "trait_info['TraitExp'] = trait_info['Disease trait'].apply(lambda x: replace_acronyms(x, all_acronyms))\n",
    "trait_info['MappedExp'] = trait_info['EFO term'].apply(lambda x: replace_acronyms(x, all_acronyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:18:58.188037Z",
     "start_time": "2025-07-03T17:16:12.226995Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1807it [02:45, 10.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# identify the best matches\n",
    "for i, row in tqdm(multi_match.iterrows()):\n",
    "    match_dict[row['Disease trait']] = map_trait(row['Disease trait'], trait_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:18:58.444242Z",
     "start_time": "2025-07-03T17:18:58.232181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make data frame from match_dict\n",
    "match_df = pd.DataFrame(match_dict).T.reset_index()\n",
    "match_df.columns = ['DISEASE/TRAIT', 'MAPPED_TRAIT_CLEAN', 'TRAIT_CODE_CLEAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:18:58.514794Z",
     "start_time": "2025-07-03T17:18:58.484896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated mappings: 3138\n",
      "Retained mappings: 16314\n"
     ]
    }
   ],
   "source": [
    "clean_study_info = study_info.merge(match_df, on='DISEASE/TRAIT', how='left')\n",
    "print('Updated mappings:', clean_study_info[clean_study_info.MAPPED_TRAIT_URI != clean_study_info.TRAIT_CODE_CLEAN].shape[0])\n",
    "print('Retained mappings:',clean_study_info[clean_study_info.MAPPED_TRAIT_URI == clean_study_info.TRAIT_CODE_CLEAN].shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_study_info.to_csv(os.path.join(datadir,'Reference_Data',  'cleaned_gwas-catalog-v1.0.3.1-studies-r2025-03-26.tsv.update'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Association Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:38:35.660847Z",
     "start_time": "2025-07-03T18:38:35.656416Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_gwas_catalog_data(datafile, outfile, pval_th=5e-8, include_intergenic=False):\n",
    "    \"\"\"Clean the GWAS Catalog data and write to a new file.\n",
    "\n",
    "    Args:\n",
    "        datafile (str): file path for GWAS Catalog data\n",
    "        outfile (str): output file for cleaned data\n",
    "        pval_th (float): p-value threshold for filtering\n",
    "        include_intergenic (bool): whether to include intergenic associations\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cols= ['DATE', 'PUBMEDID', 'DISEASE/TRAIT', 'MAPPED_GENE', 'SNP_GENE_IDS', 'P-VALUE', 'OR or BETA' ,'MAPPED_TRAIT', \n",
    "           'MAPPED_TRAIT_URI', 'INTERGENIC', 'STUDY ACCESSION', 'SNP_ID_CURRENT', 'INITIAL SAMPLE SIZE', 'GENOTYPING TECHNOLOGY']    \n",
    "    if include_intergenic:\n",
    "        cols = cols + ['UPSTREAM_GENE_ID', 'DOWNSTREAM_GENE_ID', 'UPSTREAM_GENE_DISTANCE', 'DOWNSTREAM_GENE_DISTANCE']\n",
    "    data = pd.read_csv(datafile, sep=\"\\t\", usecols=cols, low_memory=False)\n",
    "    # filter on pval\n",
    "    data = data[data[\"P-VALUE\"] <= pval_th]\n",
    "    # filter on gene and trait present\n",
    "    data = data.dropna(subset=['SNP_GENE_IDS', \"MAPPED_TRAIT_URI\"])\n",
    "    # filter out intergenic\n",
    "    if not include_intergenic:\n",
    "        data = data[data[\"INTERGENIC\"] == 0]\n",
    "    # remove associations with multiple genes\n",
    "    data = data[~data[\"SNP_GENE_IDS\"].str.contains(\",\")]\n",
    "    # remove associations with multiple traits\n",
    "    data = data[~data[\"MAPPED_TRAIT_URI\"].str.contains(\",\")]\n",
    "    # create trait code\n",
    "    data['TRAIT_CODE'] = data['MAPPED_TRAIT_URI'].apply(lambda x: x.split('/')[-1])\n",
    "    # write the cleaned file\n",
    "    data.to_csv(outfile, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:39:03.610543Z",
     "start_time": "2025-07-03T18:38:55.769567Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_gwas_catalog_data(os.path.join(datadir, 'Reference_Data', 'gwas_catalog_Jan29_2025.txt.gz'), \n",
    "                        os.path.join(datadir, 'Reference_Data', 'gwas_catalog_Jan29_2025.txt.cleaned.update'), pval_th=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:53:17.765522Z",
     "start_time": "2025-07-03T18:53:16.828745Z"
    }
   },
   "outputs": [],
   "source": [
    "gwas_genes= pd.read_csv(os.path.join(datadir, 'Reference_Data', 'gwas_catalog_Jan29_2025.txt.cleaned.gz'), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:54:50.113733Z",
     "start_time": "2025-07-03T18:53:19.093141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query batch 0 - 1000\n",
      "Query batch 1000 - 2000\n",
      "Query batch 2000 - 3000\n",
      "Query batch 3000 - 4000\n",
      "Query batch 4000 - 5000\n",
      "Query batch 5000 - 6000\n",
      "Query batch 6000 - 7000\n",
      "Query batch 7000 - 8000\n",
      "Query batch 8000 - 9000\n",
      "Query batch 9000 - 10000\n",
      "Query batch 10000 - 11000\n",
      "Query batch 11000 - 12000\n",
      "Query batch 12000 - 13000\n",
      "Query batch 13000 - 14000\n",
      "Query batch 14000 - 15000\n",
      "Query batch 15000 - 16000\n",
      "Query batch 16000 - 17000\n",
      "Query batch 17000 - 17133\n"
     ]
    }
   ],
   "source": [
    "# First map from Ensembl\n",
    "ensembl_map, missing = qe.get_latest_ensembl_id(gwas_genes['SNP_GENE_IDS'].unique())\n",
    "ensembl_to_entrez, missing_entrez = gm.convert_node_ids(ensembl_map['to'].values, 'Ensembl', 'Entrez')\n",
    "ensembl_map['Entrez'] = [ensembl_to_entrez[x] if x in ensembl_to_entrez else '' for x in ensembl_map['to']]\n",
    "id_ensembl = gwas_genes.merge(ensembl_map.loc[:, ('from', 'Entrez')], left_on='SNP_GENE_IDS', right_on='from', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:55:02.865850Z",
     "start_time": "2025-07-03T18:54:50.237328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Ids 1108\n",
      "Checking approved symbols\n",
      "Response received\n",
      "Check names 5\n",
      "Previous Ids 5\n",
      "Checking previous symbols\n",
      "Alias Ids 2\n",
      "Searching aliases\n",
      "Searching Entrez\n"
     ]
    }
   ],
   "source": [
    "# Try mapping based on symbols for unsuccessful conversions \n",
    "symbol_map, symbol_missing = qh.perform_hgnc_query(id_ensembl[(id_ensembl['Entrez'].isnull()) | (id_ensembl['Entrez']== '')]['MAPPED_GENE'].unique(), 'Symbol', 'Symbol')\n",
    "symbol_to_entrez, missing = gm.convert_node_ids(list(symbol_map.values()), 'Symbol', 'Entrez')\n",
    "symbol_map = pd.DataFrame(symbol_map.items(), columns=['from', 'to'])\n",
    "symbol_map['Entrez'] = [symbol_to_entrez[x] if x in symbol_to_entrez else '' for x in symbol_map['to']]\n",
    "id_symbol = gwas_genes.iloc[~id_ensembl.index].merge(symbol_map.loc[:, ('from', 'Entrez')], left_on='MAPPED_GENE', right_on='from', how='inner')\n",
    "id_ensembl = id_ensembl[(id_ensembl['Entrez'] != '') & (~id_ensembl['Entrez'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:55:06.235860Z",
     "start_time": "2025-07-03T18:55:02.990009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put all together\n",
    "converted_gwas_genes = pd.concat([id_ensembl, id_symbol])\n",
    "converted_gwas_genes = converted_gwas_genes[converted_gwas_genes['Entrez'] != '']\n",
    "converted_gwas_genes.to_csv(os.path.join(datadir, 'Reference_Data', 'gwas_catalog_Jan29_2025.txt.cleaned.entrez.update'), \n",
    "                                sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAVAR - Rare Variant Associations\n",
    "\n",
    "### Initial study filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:56:55.148659Z",
     "start_time": "2025-07-03T17:56:55.128860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total initial studies: 2921\n",
      "- Excluded studies 2864\n",
      "- Missing N: 2864\n"
     ]
    }
   ],
   "source": [
    "# This study information includes manually curated sample size and cohort information\n",
    "rv_study_info = pd.read_csv(os.path.join(datadir, 'Reference_Data','rv_study_info_cleaned_with_manual.tsv'), sep='\\t', index_col=0)\n",
    "print('Total initial studies:', len(rv_study_info))\n",
    "# Exclude reviews and other excluded studies\n",
    "rv_study_info = rv_study_info[(~rv_study_info.COHORT.isin(['Review', 'Exclude'])) &  (rv_study_info['Classification']!='Exclude')]\n",
    "print('- Excluded studies',  rv_study_info.shape[0])\n",
    "# exclude studies with missing sample size information\n",
    "rv_study_info = rv_study_info.dropna(subset=['N'])\n",
    "print('- Missing N:',  len( rv_study_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:57:17.959422Z",
     "start_time": "2025-07-03T17:57:17.932437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total initial traits: 2005\n"
     ]
    }
   ],
   "source": [
    "# Load provided trait information\n",
    "rv_trait_info = pd.read_csv(os.path.join(datadir,'Reference_Data', 'trait_allinfo_06112024.txt'), sep='\\t')\n",
    "print('Total initial traits:', len(rv_trait_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trait mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T17:57:50.421637Z",
     "start_time": "2025-07-03T17:57:50.403762Z"
    }
   },
   "outputs": [],
   "source": [
    "# expand acronyms in both datasets\n",
    "rv_study_info['Trait'] = rv_study_info['Reported Trait'].apply(lambda z: z.lower())\n",
    "rv_study_info['Mapped'] = rv_study_info['Trait Label'].apply(lambda z: z.lower())\n",
    "rv_study_info['TraitExp'] = rv_study_info['Reported Trait'].apply(lambda x: replace_acronyms(x, all_acronyms).lower())\n",
    "rv_study_info['MappedExp'] = rv_study_info['Trait Label'].apply(lambda x: replace_acronyms(x, all_acronyms).lower())\n",
    "rv_trait_info['Mapped'] = rv_trait_info['Trait Label'].apply(lambda x: replace_acronyms(x, all_acronyms).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:02:08.883783Z",
     "start_time": "2025-07-03T18:02:08.106315Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get GWAS mappings (clean and filtered)\n",
    "clean_study_info = pd.read_csv(os.path.join(datadir, 'Reference_Data', 'cleaned_gwas-catalog-v1.0.3.1-studies-r2025-03-26.tsv.gz'), sep='\\t').dropna(subset='MAPPED_TRAIT_CLEAN')\n",
    "clean_study_info['Trait'] = clean_study_info['DISEASE/TRAIT'].apply(lambda z: z.lower())\n",
    "clean_study_info['Mapped'] = clean_study_info['MAPPED_TRAIT_CLEAN'].apply(lambda z: z.lower())\n",
    "# (original)\n",
    "gwas_trait_info = pd.read_csv(os.path.join(datadir, 'Reference_Data' , 'gwas_catalog_trait-mappings_r2025-03-26.tsv.gz'), sep='\\t')\n",
    "gwas_trait_info['Trait'] = gwas_trait_info['Disease trait'].apply(lambda z: z.lower())\n",
    "gwas_trait_info['Mapped'] = gwas_trait_info['EFO term'].apply(lambda z: z.lower())\n",
    "gwas_trait_info['TraitExp'] = gwas_trait_info['Disease trait'].apply(lambda x: replace_acronyms(x, all_acronyms).lower())\n",
    "gwas_trait_info['MappedExp'] = gwas_trait_info['EFO term'].apply(lambda x: replace_acronyms(x, all_acronyms).lower())\n",
    "gwas_trait_info = gwas_trait_info[gwas_trait_info['Mapped'].isin(clean_study_info['Mapped'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:07:01.100816Z",
     "start_time": "2025-07-03T18:06:53.686878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact trait matches: 342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 342/342 [00:07<00:00, 46.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Identify exact trait description matches between RAVAR and GWAS Catalog\n",
    "matches = set(gwas_trait_info['Trait']).intersection(set(rv_study_info['Trait']))\n",
    "print('Exact trait matches:', len(matches))\n",
    "# add to match dictionary\n",
    "match_dict = {}\n",
    "for txt in tqdm(matches):\n",
    "    match_dict[txt] = map_trait(txt, gwas_trait_info, usecol='Trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:07:06.555026Z",
     "start_time": "2025-07-03T18:07:01.168056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches to MAPPED GWAS traits: 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [00:05<00:00, 25.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Identify exact matches between RAVAR traits and Mapped GWAS traits\n",
    "matches2 = set(gwas_trait_info['Mapped'].values).intersection(set(rv_study_info[~rv_study_info.Trait.isin(matches)]['Trait'].values))\n",
    "print('Matches to MAPPED GWAS traits:', len(matches2))\n",
    "# add to trait dictionary\n",
    "for txt in tqdm(matches2):\n",
    "     match_dict[txt] = map_trait(txt, gwas_trait_info, usecol='Mapped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:07:37.913935Z",
     "start_time": "2025-07-03T18:07:37.911644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches so far: 480\n"
     ]
    }
   ],
   "source": [
    "matched_traits = matches.union(matches2)\n",
    "print('Total matches so far:', len(matched_traits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:08:28.669741Z",
     "start_time": "2025-07-03T18:08:24.523374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches to MAPPED RAVAR traits: 193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 46.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Identify matches between GWAS traits and mapped RAVAR traits\n",
    "matches3 = set(gwas_trait_info['Trait'].values).intersection(set(rv_study_info[~rv_study_info.Trait.isin(matched_traits)]['Mapped'].values))\n",
    "print('Matches to MAPPED RAVAR traits:',len(matches3))\n",
    "map_dict = {}\n",
    "for txt in tqdm(matches3):\n",
    "     map_dict[txt] = map_trait(txt, gwas_trait_info, usecol='Trait')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:08:46.113022Z",
     "start_time": "2025-07-03T18:08:46.110783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches so far: 673\n"
     ]
    }
   ],
   "source": [
    "print('Total matches so far:', len(match_dict) + len(map_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:10:06.384680Z",
     "start_time": "2025-07-03T18:10:06.367794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a final dataset of trait matches\n",
    "trait_df = pd.DataFrame(match_dict).T.reset_index()\n",
    "trait_df.columns = ['Trait', 'MAPPED_TRAIT_CLEAN', 'TRAIT_CODE_CLEAN']\n",
    "trait_df['TRAIT_CODE_CLEAN'] = trait_df['TRAIT_CODE_CLEAN'].apply(lambda x: x.split('/')[-1])\n",
    "# And trait mappings\n",
    "mapped_df = pd.DataFrame(map_dict).T.reset_index()\n",
    "mapped_df.columns = ['Mapped', 'MAPPED_TRAIT_CLEAN', 'TRAIT_CODE_CLEAN']\n",
    "mapped_df['TRAIT_CODE_CLEAN'] = mapped_df['TRAIT_CODE_CLEAN'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final RV study information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:11:07.694102Z",
     "start_time": "2025-07-03T18:11:07.686628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mapped: 1165\n"
     ]
    }
   ],
   "source": [
    "# Combine results\n",
    "rv_study_info1 = rv_study_info.merge(trait_df, on='Trait', how='inner')\n",
    "rv_study_info2 = rv_study_info[~rv_study_info.rv_idx.isin(rv_study_info1.rv_idx.values)].merge(mapped_df, on='Mapped', how='inner')\n",
    "rv_study_info_mapped = pd.concat([rv_study_info1, rv_study_info2])\n",
    "print('Total mapped:', len(rv_study_info_mapped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:11:25.894972Z",
     "start_time": "2025-07-03T18:11:25.889778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining: 1699\n"
     ]
    }
   ],
   "source": [
    "# For any traits not included about, retain the original mappings\n",
    "rv_study_info_remaining = rv_study_info[~rv_study_info.rv_idx.isin(rv_study_info_mapped.rv_idx.values)].copy()\n",
    "rv_study_info_remaining['MAPPED_TRAIT_CLEAN'] = rv_study_info_remaining['Mapped']\n",
    "rv_study_info_remaining['TRAIT_CODE_CLEAN'] = rv_study_info_remaining['Trait Ontology id'].apply(lambda x: x.replace(':', '_'))\n",
    "print('Total remaining:', len(rv_study_info_remaining))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:11:44.288743Z",
     "start_time": "2025-07-03T18:11:44.286223Z"
    }
   },
   "outputs": [],
   "source": [
    "rv_study_info_out = pd.concat([rv_study_info_mapped, rv_study_info_remaining])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:14:40.594222Z",
     "start_time": "2025-07-03T18:14:40.453814Z"
    }
   },
   "outputs": [],
   "source": [
    "rv_study_info_out.to_csv(os.path.join(datadir, 'Reference_Data','rv_study_info_cleaned_with_manual_mapped_Mar28.tsv.update'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RV Association Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:15:13.614031Z",
     "start_time": "2025-07-03T18:15:13.597759Z"
    }
   },
   "outputs": [],
   "source": [
    "rv_study_info = pd.read_csv(os.path.join(datadir,'Reference_Data', 'rv_study_info_cleaned_with_manual_mapped_Mar28.tsv'),\n",
    "                            sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:31:26.726573Z",
     "start_time": "2025-07-03T18:31:25.850067Z"
    }
   },
   "outputs": [],
   "source": [
    "ravar_genes = pd.read_csv(os.path.join(datadir,'Reference_Data' ,'gene_fulltable_06112024.txt.gz'),sep='\\t', \n",
    "                            usecols=['Gene Symbol', 'Ensembl ID', 'Gene Type', 'CHR', 'Location', 'Reported Trait', \n",
    "                                     'Trait Label', 'Trait Ontology id', 'EFO synonym', 'P-value', 'PMID'],\n",
    "                         low_memory=False)\n",
    "#replace '−' with '-'\n",
    "ravar_genes['P-value'] = ravar_genes['P-value'].apply(lambda x: float(x.replace('−','-')) if type(x) == str else float(x))\n",
    "ravar_genes['TRAIT_CODE'] = ravar_genes['Trait Ontology id'].apply(lambda x: x.replace(\":\", \"_\") if type(x) == str else x)\n",
    "ravar_genes['logp'] = -1 * np.log10(ravar_genes['P-value'] + 1e-250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:31:26.838908Z",
     "start_time": "2025-07-03T18:31:26.824084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reported traits 3049\n",
      "Total traits with Ontology 1468\n",
      "Total genes 12850\n",
      "Unique studies 200\n",
      "Min p-value 0.0\n",
      "Max p-value 9.94e-05\n"
     ]
    }
   ],
   "source": [
    "print('Reported traits', len(ravar_genes['Reported Trait'].unique()))\n",
    "print('Total traits with Ontology', len(ravar_genes['Trait Ontology id'].unique()))\n",
    "print('Total genes', len(ravar_genes['Gene Symbol'].unique()))\n",
    "print('Unique studies', len(ravar_genes['PMID'].unique()))\n",
    "print('Min p-value', ravar_genes['P-value'].min())\n",
    "print('Max p-value', ravar_genes['P-value'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:27:19.263316Z",
     "start_time": "2025-07-03T18:26:07.648258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query batch 0 - 1000\n",
      "Query batch 1000 - 2000\n",
      "Query batch 2000 - 3000\n",
      "Query batch 3000 - 4000\n",
      "Query batch 4000 - 5000\n",
      "Query batch 5000 - 6000\n",
      "Query batch 6000 - 7000\n",
      "Query batch 7000 - 8000\n",
      "Query batch 8000 - 9000\n",
      "Query batch 9000 - 10000\n",
      "Query batch 10000 - 11000\n",
      "Query batch 11000 - 12000\n",
      "Query batch 12000 - 12850\n"
     ]
    }
   ],
   "source": [
    "ensembl_map, missing = qe.get_latest_ensembl_id(ravar_genes['Ensembl ID'].unique())\n",
    "ensembl_to_entrez, missing_entrez = gm.convert_node_ids(ensembl_map['to'].values, 'Ensembl', 'Entrez')\n",
    "ensembl_map['Entrez'] = [ensembl_to_entrez[x] if x in ensembl_to_entrez else '' for x in ensembl_map['to']]\n",
    "id_ensembl = ravar_genes.merge(ensembl_map.loc[:, ('from', 'Entrez')], left_on='Ensembl ID', right_on='from', how='inner')\n",
    "id_ensembl = id_ensembl[id_ensembl['Entrez'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:33:51.297130Z",
     "start_time": "2025-07-03T18:33:44.824868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Ids 59\n",
      "Checking approved symbols\n",
      "Response received\n",
      "Check names 3\n",
      "Previous Ids 2\n",
      "Checking previous symbols\n"
     ]
    }
   ],
   "source": [
    "if len(missing_entrez) > 0:\n",
    "    symbol_map, symbol_missing = qh.perform_hgnc_query(ravar_genes[ravar_genes['Ensembl ID'].isin(missing_entrez)]['Gene Symbol'].unique(), 'Symbol', 'Symbol')\n",
    "    symbol_to_entrez, missing = gm.convert_node_ids(list(symbol_map.values()), 'Symbol', 'Entrez')\n",
    "    symbol_map = pd.DataFrame(symbol_map.items(), columns=['from', 'to'])\n",
    "    symbol_map['Entrez'] = [symbol_to_entrez[x] if x in symbol_to_entrez else '' for x in symbol_map['to']]\n",
    "    id_symbol = ravar_genes.iloc[~id_ensembl.index].merge(symbol_map.loc[:, ('from', 'Entrez')], left_on='Gene Symbol', right_on='from', how='inner')\n",
    "    converted_ravar_genes = pd.concat([id_ensembl, id_symbol])\n",
    "else:\n",
    "    converted_ravar_genes = id_ensembl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the PMCIDs to the converted data\n",
    "converted_ravar_genes = converted_ravar_genes.merge(ravar_genes.loc[:, ('PMID', 'Ensembl ID', 'Trait Label', 'P-value', 'Reported Trait', 'Location',\n",
    "                        'Gene Symbol')].drop_duplicates(), on=['Gene Symbol', 'Ensembl ID', 'Trait Label', 'P-value', 'Reported Trait', 'Location'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T18:35:34.854588Z",
     "start_time": "2025-07-03T18:35:34.217262Z"
    }
   },
   "outputs": [],
   "source": [
    "converted_ravar_genes.to_csv(os.path.join(datadir,'Reference_Data' ,'gene_fulltable_06112024.txt.entrez.update'), sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CARVA)",
   "language": "python",
   "name": "carva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "240.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
